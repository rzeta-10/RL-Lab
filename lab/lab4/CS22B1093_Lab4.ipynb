{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RL Lab Assignment - 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CS22B1093 Rohan G"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement SARSA and Q-Learning using appropriate update functions for Frozen Lake environment.\n",
    "### Additionally, implement the same using any environment of choice (refer gymnasium documentation)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Frozen Lake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training SARSA on FrozenLake...\n",
      "SARSA Q-Table:\n",
      " [[0.70192616 0.79955915 0.67809948 0.69903331]\n",
      " [0.73445844 0.         0.54812374 0.56956583]\n",
      " [0.55043742 0.32666845 0.09450037 0.44302905]\n",
      " [0.32747536 0.         0.         0.        ]\n",
      " [0.70153063 0.81834821 0.         0.72266045]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.9201008  0.         0.44493749]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.78332527 0.         0.94038028 0.7401598 ]\n",
      " [0.82016871 0.9629409  0.81895418 0.        ]\n",
      " [0.92935132 0.98966197 0.         0.8635999 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.97451909 0.98478742 0.89674206]\n",
      " [0.97136031 0.98851343 1.         0.94985392]\n",
      " [0.         0.         0.         0.        ]]\n",
      "\n",
      "Training Q-Learning on FrozenLake...\n",
      "Q-Learning Q-Table:\n",
      " [[0.94148015 0.95099005 0.93206535 0.94148015]\n",
      " [0.94148015 0.         0.5116827  0.67801647]\n",
      " [0.80452842 0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.95099005 0.96059601 0.         0.94148015]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.92719011 0.         0.06851613]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.96059601 0.         0.970299   0.95099005]\n",
      " [0.96059601 0.9801     0.96059601 0.        ]\n",
      " [0.970299   0.94336973 0.         0.82680004]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.9801     0.99       0.970299  ]\n",
      " [0.9801     0.99       1.         0.96059601]\n",
      " [0.         0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "def epsilon_greedy(Q, state, epsilon, n_actions):\n",
    "    if np.random.rand() < epsilon:\n",
    "        return np.random.randint(n_actions)  # Random action\n",
    "    return np.argmax(Q[state])  # Best action\n",
    "\n",
    "def sarsa(env, alpha=0.1, gamma=0.99, epsilon=0.1, episodes=100000):\n",
    "    Q = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "    for episode in range(episodes):\n",
    "        state, _ = env.reset()  # Reset environment; state is the first element.\n",
    "        action = epsilon_greedy(Q, state, epsilon, env.action_space.n)\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            next_state, reward, done, truncated, info = env.step(action)\n",
    "            next_action = epsilon_greedy(Q, next_state, epsilon, env.action_space.n)\n",
    "            # SARSA update rule\n",
    "            Q[state, action] += alpha * (reward + gamma * Q[next_state, next_action] - Q[state, action])\n",
    "            state, action = next_state, next_action  # Move to next state and action\n",
    "            \n",
    "    return Q\n",
    "\n",
    "def q_learning(env, alpha=0.1, gamma=0.99, epsilon=0.1, episodes=100000):\n",
    "    Q = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "    for episode in range(episodes):\n",
    "        state, _ = env.reset()  # Reset environment; state is the first element.\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            action = epsilon_greedy(Q, state, epsilon, env.action_space.n)\n",
    "            next_state, reward, done, truncated, info = env.step(action)\n",
    "            # Q-Learning update rule\n",
    "            Q[state, action] += alpha * (reward + gamma * np.max(Q[next_state]) - Q[state, action])\n",
    "            state = next_state  # Move to next state\n",
    "            \n",
    "    return Q\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Create a deterministic FrozenLake environment.\n",
    "    env = gym.make(\"FrozenLake-v1\", is_slippery=False)\n",
    "    \n",
    "    print(\"Training SARSA on FrozenLake...\")\n",
    "    Q_sarsa = sarsa(env, alpha=0.1, gamma=0.99, epsilon=0.1, episodes=100000)\n",
    "    print(\"SARSA Q-Table:\\n\", Q_sarsa)\n",
    "    \n",
    "    # Re-create the environment for Q-Learning training to start fresh.\n",
    "    env = gym.make(\"FrozenLake-v1\", is_slippery=False)\n",
    "    \n",
    "    print(\"\\nTraining Q-Learning on FrozenLake...\")\n",
    "    Q_qlearning = q_learning(env, alpha=0.1, gamma=0.99, epsilon=0.1, episodes=100000)\n",
    "    print(\"Q-Learning Q-Table:\\n\", Q_qlearning)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CliffWalker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training SARSA on CliffWalking...\n",
      "Final SARSA Q-Table:\n",
      " [[ -15.37899782  -14.65329748  -16.68538982  -15.33825961]\n",
      " [ -14.37660531  -13.47268809  -15.80218594  -15.55899291]\n",
      " [ -13.50303413  -12.53734656  -13.42975245  -14.58210229]\n",
      " [ -12.35614642  -11.4540435   -12.30718624  -13.87359431]\n",
      " [ -11.42098761  -10.44402317  -11.72310666  -12.62525724]\n",
      " [ -10.41764245   -9.61452071  -10.61973609  -11.6653004 ]\n",
      " [  -9.50395937   -8.70745762   -9.04253092  -10.64137781]\n",
      " [  -8.43115768   -7.56530153   -7.86573275   -9.79052122]\n",
      " [  -7.39079848   -6.44801526   -6.66467794   -8.56249009]\n",
      " [  -6.38598471   -5.5448607    -6.09238931   -7.53796622]\n",
      " [  -5.32392208   -4.56923783   -4.45814472   -6.5490896 ]\n",
      " [  -4.32324294   -4.28404454   -3.50231496   -5.7500307 ]\n",
      " [ -15.2703113   -14.72309962  -18.13914245  -16.09325941]\n",
      " [ -14.69202988  -12.86031509  -32.77461631  -15.77509948]\n",
      " [ -13.43992087  -12.17633846  -17.21482228  -15.08268657]\n",
      " [ -12.4828454   -11.13094785  -18.90623887  -13.39076366]\n",
      " [ -11.89306839  -10.11616747  -12.36768032  -12.5569792 ]\n",
      " [ -10.44984514   -8.91247269  -14.68536358  -11.92569187]\n",
      " [  -9.77037123   -7.78793715  -10.46548451  -10.32226087]\n",
      " [  -8.45732485   -6.40514508   -9.11450787   -9.14096236]\n",
      " [  -7.41911288   -5.1598431    -8.17590448   -8.08903848]\n",
      " [  -6.67538205   -4.45566724  -14.78016577   -7.06108553]\n",
      " [  -5.41980022   -3.20408354   -4.13845429   -5.57029684]\n",
      " [  -4.42597874   -3.19169849   -2.02913757   -4.2774135 ]\n",
      " [ -16.01193361  -18.63798295  -18.22284655  -17.33179587]\n",
      " [ -14.22976725  -16.8976979  -122.17533335  -17.26928875]\n",
      " [ -13.26596223  -14.29478132 -112.62741925  -15.50439282]\n",
      " [ -12.60659657  -13.56876616  -99.39425174  -17.89795238]\n",
      " [ -11.17850592  -12.17528176 -100.27117548  -12.78777374]\n",
      " [ -10.34308867  -14.22093787  -91.55099717  -12.82553055]\n",
      " [  -9.03078525  -12.0157257   -88.92958378  -14.75325086]\n",
      " [  -7.79055073  -10.11048459 -109.84541958   -9.60351087]\n",
      " [  -8.03708356   -6.65379621  -96.2601981    -9.51195946]\n",
      " [  -5.4820422    -8.43370746 -106.35049936   -7.89334382]\n",
      " [  -5.35580681   -2.00514533 -121.83224783  -11.26843152]\n",
      " [  -3.36469628   -2.13486307   -1.           -3.03555326]\n",
      " [ -17.09496057 -117.32789436  -30.04031455  -18.56429809]\n",
      " [   0.            0.            0.            0.        ]\n",
      " [   0.            0.            0.            0.        ]\n",
      " [   0.            0.            0.            0.        ]\n",
      " [   0.            0.            0.            0.        ]\n",
      " [   0.            0.            0.            0.        ]\n",
      " [   0.            0.            0.            0.        ]\n",
      " [   0.            0.            0.            0.        ]\n",
      " [   0.            0.            0.            0.        ]\n",
      " [   0.            0.            0.            0.        ]\n",
      " [   0.            0.            0.            0.        ]\n",
      " [   0.            0.            0.            0.        ]]\n",
      "\n",
      "Training Q-Learning on CliffWalking...\n",
      "Final Q-Learning Q-Table:\n",
      " [[ -12.8557859   -12.86447262  -12.88603997  -12.84310252]\n",
      " [ -12.32669758  -12.23180203  -12.23258379  -12.36404272]\n",
      " [ -11.50764343  -11.36095148  -11.36094669  -11.70289345]\n",
      " [ -10.71214699  -10.46614891  -10.46614969  -11.1468329 ]\n",
      " [ -10.16435112   -9.561792     -9.56179197  -10.35479989]\n",
      " [  -9.07581211   -8.64827523   -8.64827523   -9.5061574 ]\n",
      " [  -8.1502318    -7.72553055   -7.72553055   -8.72843026]\n",
      " [  -7.40931212   -6.79346521   -6.79346521   -7.77886987]\n",
      " [  -6.59874088   -5.85198506   -5.85198506   -7.25425072]\n",
      " [  -5.6994844    -4.90099501   -4.90099501   -6.413197  ]\n",
      " [  -4.71760038   -3.940399     -3.940399     -5.35199322]\n",
      " [  -3.81555739   -3.78749725   -2.9701       -4.47886612]\n",
      " [ -13.50939819  -12.2478977   -12.2478977   -13.12242128]\n",
      " [ -13.08530705  -11.36151283  -11.36151283  -13.12523565]\n",
      " [ -12.24592638  -10.46617457  -10.46617457  -12.24787007]\n",
      " [ -11.36139926   -9.5617925    -9.5617925   -11.36151122]\n",
      " [ -10.46617263   -8.64827525   -8.64827525  -10.46617452]\n",
      " [  -9.5617923    -7.72553056   -7.72553056   -9.5617925 ]\n",
      " [  -8.64827522   -6.79346521   -6.79346521   -8.64827525]\n",
      " [  -7.72553056   -5.85198506   -5.85198506   -7.72553056]\n",
      " [  -6.79346521   -4.90099501   -4.90099501   -6.79346521]\n",
      " [  -5.85198506   -3.940399     -3.940399     -5.85198506]\n",
      " [  -4.90099501   -2.9701       -2.9701       -4.90099501]\n",
      " [  -3.940399     -2.9701       -1.99         -3.940399  ]\n",
      " [ -13.12541872  -11.36151283  -13.12541872  -12.2478977 ]\n",
      " [ -12.2478977   -10.46617457 -112.12541872  -12.2478977 ]\n",
      " [ -11.36151283   -9.5617925  -112.12541872  -11.36151283]\n",
      " [ -10.46617457   -8.64827525 -112.12541872  -10.46617457]\n",
      " [  -9.5617925    -7.72553056 -112.12541872   -9.5617925 ]\n",
      " [  -8.64827525   -6.79346521 -112.12541872   -8.64827525]\n",
      " [  -7.72553056   -5.85198506 -112.12541872   -7.72553056]\n",
      " [  -6.79346521   -4.90099501 -112.12541872   -6.79346521]\n",
      " [  -5.85198506   -3.940399   -112.12541872   -5.85198506]\n",
      " [  -4.90099501   -2.9701     -112.12541872   -4.90099501]\n",
      " [  -3.940399     -1.99       -112.12541872   -3.940399  ]\n",
      " [  -2.9701       -1.99         -1.           -2.9701    ]\n",
      " [ -12.2478977  -112.12541872  -13.12541872  -13.12541872]\n",
      " [   0.            0.            0.            0.        ]\n",
      " [   0.            0.            0.            0.        ]\n",
      " [   0.            0.            0.            0.        ]\n",
      " [   0.            0.            0.            0.        ]\n",
      " [   0.            0.            0.            0.        ]\n",
      " [   0.            0.            0.            0.        ]\n",
      " [   0.            0.            0.            0.        ]\n",
      " [   0.            0.            0.            0.        ]\n",
      " [   0.            0.            0.            0.        ]\n",
      " [   0.            0.            0.            0.        ]\n",
      " [   0.            0.            0.            0.        ]]\n"
     ]
    }
   ],
   "source": [
    "def epsilon_greedy(Q, state, epsilon, n_actions):\n",
    "    if np.random.rand() < epsilon:\n",
    "        return np.random.randint(n_actions)  # Random action (could be 0)\n",
    "    return np.argmax(Q[state])  # Best (greedy) action\n",
    "\n",
    "def sarsa(env, alpha=0.1, gamma=0.99, epsilon=0.1, episodes=50000, print_interval=1):\n",
    "    Q = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "    \n",
    "    for episode in range(episodes):\n",
    "        state, _ = env.reset()  # Reset the environment; state is the first element.\n",
    "        action = epsilon_greedy(Q, state, epsilon, env.action_space.n)\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            next_state, reward, done, truncated, info = env.step(action)\n",
    "            next_action = epsilon_greedy(Q, next_state, epsilon, env.action_space.n)\n",
    "            # SARSA update rule:\n",
    "            Q[state, action] += alpha * (reward + gamma * Q[next_state, next_action] - Q[state, action])\n",
    "            state, action = next_state, next_action  # Move to next state and action\n",
    "    \n",
    "    return Q\n",
    "\n",
    "def q_learning(env, alpha=0.1, gamma=0.99, epsilon=0.1, episodes=50000, print_interval=1):\n",
    "    Q = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "    \n",
    "    for episode in range(episodes):\n",
    "        state, _ = env.reset()  # Reset the environment; state is the first element.\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            action = epsilon_greedy(Q, state, epsilon, env.action_space.n)\n",
    "            next_state, reward, done, truncated, info = env.step(action)\n",
    "            # Q-Learning update rule:\n",
    "            Q[state, action] += alpha * (reward + gamma * np.max(Q[next_state]) - Q[state, action])\n",
    "            state = next_state  # Move to next state\n",
    "    \n",
    "    return Q\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Create the CliffWalking environment.\n",
    "    env = gym.make(\"CliffWalking-v0\")\n",
    "    \n",
    "    print(\"Training SARSA on CliffWalking...\")\n",
    "    Q_sarsa = sarsa(env, alpha=0.1, gamma=0.99, epsilon=0.1, episodes=50000, print_interval=1)\n",
    "    print(\"Final SARSA Q-Table:\\n\", Q_sarsa)\n",
    "    \n",
    "    # Re-create the environment for Q-Learning to ensure a fresh start.\n",
    "    env = gym.make(\"CliffWalking-v0\")\n",
    "    \n",
    "    print(\"\\nTraining Q-Learning on CliffWalking...\")\n",
    "    Q_qlearning = q_learning(env, alpha=0.1, gamma=0.99, epsilon=0.1, episodes=50000, print_interval=1)\n",
    "    print(\"Final Q-Learning Q-Table:\\n\", Q_qlearning)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
